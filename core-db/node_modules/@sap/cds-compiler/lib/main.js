// Main entry point for the Research Vanilla CDS Compiler
//
// File for external usage = which is read in other modules with
//   require('cdsv');

// Proposed intra-module lib dependencies:
//  - lib/base/<file>.js: can be required by all others, requires no other
//    of this project
//  - lib/<dir>/<file>.js: can be required by other files lib/<dir>/,
//    can require other files lib/<dir>/ and lib/base/<file>.js
//  - lib/main.js (this file): can be required by none in lib/ (only in
//    bin/ and test/), can require any other

'use strict';

const backends = require('./backends');

// The compiler version (taken from package.json)
function version() {
  return require('../package.json').version;
}

// The CSN file format version produced by this compiler
// (Note that all 0.x.x versions are floating targets, i.e. the format is frequently
// changed without notice. The versions only signal certain combinations of "flavors", see below
//  0.0.1  : Used by HANA CDS for its CSN output (incomplete, not well defined, quite different from CDX ...)
//  0.0.2  : CDX in the initial versions with old-style CSN and 'deep' naming for HANA/SQL
//  0.0.99 : Like 0.0.2 but with new-style CSN and 'deep' naming forHANA/SQL
//  0.1.0  : Like 0.0.2 but with old-style CSN and 'flat' naming for HANA/SQL
//  0.1.99 : Like 0.0.2 but with new-style CSN and 'flat' naming for HANA/SQL
// FIXME: This is kind of weird and should not stay like this. Currently, your best guess for versions
// having initial digit '0' is the following rule:
// - if last digit is '99', you have new-style CSN, otherwise old-style CSN
// - if middle digit is 0, you have 'deep' names, otherwise 'flat' names for HANA/SQL
function csnVersion( options ) {
  // Merge default options
  options = mergeOptions(backends.getDefaultBackendOptions(), options);

  // New-style CSN
  if (options.newCsn) {
    return (options.toHana.names == 'deep' || options.toSql.names == 'deep') ? '0.0.99' : '0.1.99';
  }
  // Old-style CSN
  else {
    return (options.toHana.names == 'deep' || options.toSql.names == 'deep') ? '0.0.2' : '0.1.0';
  }
}

var { CompilationError, messageString, sortMessages, hasErrors } = require('./base/messages');
var { promiseAllDoNotRejectImmediately } = require('./base/node-helpers');

var parseLanguage = require('./language/antlrParser');
const moduleLayers = require('./compiler/moduleLayers');
var define = require('./compiler/definer');
var resolve = require('./compiler/resolver');
var propagator = require('./compiler/propagator');
const semanticChecks = require('./checks/semanticChecks');
var compactJson = require('./json/compactor').compact;
var compactSortedJson = require('./json/compactor').compactSorted;
const { compactModel, compactQuery, compactExpr } = require('./json/to-csn')
var augmentor = require("../lib/json/augmentor.js");
var generateExts = require('./i18n/generate-extensions'); // extensions from an properties file
var fs = require('fs');
const forHana = require('../lib/output/forHana');
var transformModel = require('./edm/transformModel');
var tntSpecific = require('./edm/tntSpecific');
var csn2edm = require('./edm/csn2edm');
const emdx2csn = require('./edm/annotations/edmx2csnNew'); // translate edmx annotations into csn
var alerts = require('./base/alerts');
const { mergeOptions } = require('../lib/model/modelUtils');

const path = require('path');
const moduleResolve = require('resolve');

const extensions = ['.cds', '.json'];

function packageFilter( pkg ) {
  return { main: pkg.cds && pkg.cds.main || pkg.main };
}

function parse( source, filename, options = {} ) {
  let ext = path.extname( filename ).toLowerCase();
  if (ext === '.properties')
    return generateExts( source, filename, options );
  else if (ext === '.xml')
    return emdx2csn( source, filename, options );
  else if (['.json', '.csn'].includes(ext)) {
    // TODO: put this into augmentor.js, having standard args ( source, filename, options )
    // parse json with locations
    let jlParser = require("./json/parse")
    let augmentedJSON = jlParser.parse( source, filename );

    let model = augmentedJSON.condense();

    // CSN validation
    let validateCSN = require("./json/schema/validateCSN.js");
    validateCSN(model);

    // augment CSN
    augmentor.augment(model , filename, options );

    model.$frontend = 'json';
    return model;
  } else if (options.fallbackParser || ['.cds', '.hdbcds', '.hdbdd'].includes(ext))
    return parseLanguage( source, filename, options );
  else {
    var model = { messages: [] };
    const { signal, error } = alerts( model );
    //console.log(`Unknown file extension '${ext}'`, filename);
    signal( error`Unknown file extension '${ext}'`,
            { filename, start: { offset: 0, line: 1, column: 1 } } );
    return model;
  }
}


// Main function: Compile the sources from the files given by the array of
// `filenames`.  As usual with the `fs` library, relative file names are
// relative to the working directory `process.cwd()`.  With argument `dir`, the
// file names are relative to `process.cwd()+dir`.  Options can have the
// following properties:
//  - Truthy `parseOnly`: stop compilation after parsing.
//  - Truthy `lintMode`: do not report errors for using directives pointing to
//    non-existing artifacts.
//  - many others - TODO

// This function returns a Promise.  See ../bin/cdsv.js for an example usage.
// See function `compileSync` or `compileSources` for alternative compile
// functions.
//
// The promise is fulfilled if all files could be read and processed without
// errors.  The fulfillment value is an augmented CSN (see
// ./compiler/definer.js).
//
// If there are errors, the promise is rejected.  If there was an invocation
// error (repeated filenames or if the file could not be read), the rejection
// value is an InvocationError.  Otherwise, the rejection value is a
// CompilationError containing a vector of individual errors.
//
// `fileCache` is a dictionary of absolute file names to the file content
//  - false: the file does not exist
//  - true: file exists (fstat), no further knowledge yet - i.e. value will change!
//  - 'string' or instanceof Buffer: the file content
//  - { realname: fs.realpath(filename) }: if filename is not canonicalized
//
function compile( filenames, dir='', options = {}, fileCache = Object.create(null) ) {
  // A non-proper dictionary (i.e. with prototype) is safe if the keys are
  // absolute file names - they start with `/` or `\` or similar
  // if (Object.getPrototypeOf( fileCache ))
  //   fileCache = Object.assign( Object.create(null), fileCache );
  dir = path.resolve(dir);
  var a = processFilenames( filenames, dir );
  a.fileContentDict = Object.create(null);

  var messagesArray = [[]];
  const { signal, error } = alerts( { messages: messagesArray[0] } );

  var all = promiseAllDoNotRejectImmediately( a.files.map(readAndParse) );

  all = all
    .then( testInvocation, function (reason) {
      // do not reject with PromiseAllError, use InvocationError:
      let errs = reason.valuesOrErrors.filter (e => e instanceof Error);
      // internal error if no file IO error (has property `path`)
      return Promise.reject( errs.find( e => !e.path ) ||
                             new InvocationError([...a.repeated, ...errs]) )
    });
  if (!options.lintMode && !options.parseOnly)
    all = all.then( readDependencies );
  return all.then( function() {
    moduleLayers.setLayers( a.sources, a.files );
    for (let name in a.sources)
      messagesArray.push( a.sources[name].messages || [] );
    return compileDo( a.sources, messagesArray, options, a.fileContentDict );
  });

  // Read file `filename` and parse its content, return messages
  function readAndParse( filename ) {
    if ( filename === false )   // module which has not been found
      return [];
    let rel = a.sources[filename] || path.relative( dir, filename );
    if (typeof rel === 'object') // already parsed
      return [];                 // no further dependency processing
    // no parallel readAndParse with same resolved filename should read the file,
    // also ensure deterministic sequence in a.sources:
    a.sources[filename] = { filename: rel };

    return new Promise( function (fulfill, reject) {
      readFile( filename, 'utf8', function (err, source) {
        if (err)
          reject(err);
        else {
          a.fileContentDict[filename] = source;
          let ast = parse( source, rel, options );
          a.sources[filename] = ast;
          ast.filename = rel;
          ast.dirname = path.dirname( filename );
          fulfill( ast );
        }
      });
    });
  }

  function readFile( filename, enc, cb ) {
    if (typeof enc === 'function') // moduleResolve uses old-style API
      cb = enc, enc = null;
    let body = fileCache[ filename ];
    if (body && typeof body === 'object' && body.realname) {
      filename = body.realname; // use fs.realpath name
      body = fileCache[ filename ];
    }
    if (body !== undefined && body !== true) { // true: we just know it is there
      if (body === false) {
        body = new Error( `ENOENT: no such file or directory, open '${filename}'`);
        body.code = 'ENOENT', body.errno = -2, body.syscall = 'open';
        body.path = filename;
      }
      if (body instanceof Error) {
        traceFS( 'READFILE:cache-error:', filename, body.message );
        cb( body )   // no need for process.nextTick( cb, body ) with moduleResolve
      }
      else {
        traceFS( 'READFILE:cache:', filename, body );
        cb( null, body );
      }
    }
    else {
      traceFS( 'READFILE:start:', filename );
      // TODO: set cache directly to some "delay" - store error differently?
      // e.g. an error of callback functions!
      fs.readFile( filename, enc, function( err, data ) {
        fileCache[ filename ] = err || data;
        traceFS( 'READFILE:data:', filename, err || data );
        cb( err, data );
      });
    }
  }

  function isFile( filename, cb ) {
    let body = fileCache[ filename ];
    if (body !== undefined) {
      traceFS( 'ISFILE:cache:', filename, body );
      if (body instanceof Error)
        cb( body )   // no need for process.nextTick( cb, body ) with moduleResolve
      else
        cb( null, !!body );
    }
    else {
      traceFS( 'ISFILE:start:', filename, body );
      // in the future (if we do module resolve ourself with just readFile),
      // we avoid parallel readFile by storing having an array of `cb`s in
      // fileCache[ filename ] before starting fs.readFile().
      fs.stat( filename, function( err, stat ) {
        if (err)
          body = (err.code === 'ENOENT' || err.code === 'ENOTDIR') ? false : err;
        else
          body = !!(stat.isFile() || stat.isFIFO());
        if (fileCache[ filename ] === undefined) // parallel readFile() has been processed
          fileCache[ filename ] = body;
        traceFS( 'ISFILE:data:', filename, body );
        if (body instanceof Error)
          cb( err );
        else
          cb( null, body );
      });
    }
  }

  function traceFS( intro, filename, data ) {
    if (options.traceFs)
      // eslint-disable-next-line no-console
      console.log( intro, filename,
                   (typeof data === 'string' || data instanceof Buffer)
                   ? typeof data
                   : (data === undefined) ? '?' : '' + data );
  }

  // Combine the parse results (if there are not file IO errors)
  function testInvocation( values ) {
    if (a.repeated.length)
      // repeated file names in invocation => just report these
      return Promise.reject( new InvocationError(a.repeated) );
    return values;
  }

  function readDependencies( astArray ) {
    let promises = [];
    for (let ast of astArray) {
      // console.log( 'READ-DEP:',ast.filename, ast.dependencies && ast.dependencies.length )
      if (!ast.dependencies || !ast.dependencies.length)
        continue;
      let dependencies = Object.create( null );
      for (let d of ast.dependencies) {
        let module = d.val;
        let dep = dependencies[module];
        if (dep)
          dep.usingFroms.push( d );
        else
          dependencies[module] = { module, basedir: ast.dirname, usingFroms: [d] };
      }
      // create promises after all usingFroms have been collected, as the
      // Promise executor is called immediately with `new`:
      for (let module in dependencies)
        promises.push( resolveModule( dependencies[module] ) );
    }
    if (!promises.length)
      return [];
    // read files (important part: adding filename to a.sources) after having
    // resolved the module names to ensure deterministic sequence in a.sources
    return Promise.all( promises )
      .then( fileNames => Promise.all( fileNames.map( readAndParse ) ) )
      .then( readDependencies );
  }

  function resolveModule( dep ) {
    // let opts = { extensions, packageFilter, basedir: dep.basedir, preserveSymlinks: false };
    // `preserveSymlinks` option does not really work -> provide workaround anyway...
    // Hm, the resolve package also does not follow the node recommendation:
    // "Using fs.stat() to check for the existence of a file before calling
    // fs.open(), fs.readFile() or fs.writeFile() is not recommended"
    let opts = { extensions, packageFilter, basedir: dep.basedir, isFile, readFile };
    return new Promise( function (fulfill, reject) {
      moduleResolve( dep.module, opts, function (err, res) {
        // console.log('RESOLVE', dep, res, err)
        if (err)
          reject(err);
        else {
          let body = fileCache[ res ];
          if (body === undefined || body === true) { // use fs if no or just temp entry
            dep.absname = res;
            fs.realpath( res, cb );
          }
          else if (body && typeof body === 'object' && body.realname) {
            //dep.absname = body.realname;
            cb( null, body.realname ); // use fs.realpath name
          }
          else {
            //dep.absname = res;
            cb( null, res );
          }
        }
      });

      function cb( err, res ) {
        if (err)
          reject(err);
        else {
          if (dep.absname)
            fileCache[ dep.absname ] = (dep.absname === res) || { realname: res };
          dep.resolved = res;   // store in dep that module resolve was successful
          for (let from of dep.usingFroms)
            from.realname = res;
          fulfill(res);
        }
      }
    })
      .catch( function() {      // (err)  TODO: check for expected exceptions
        if (dep.resolved) {
          let resolved = path.relative( dep.basedir, dep.resolved );
          if (options.testMode)
            resolved = resolved.replace( /\\/g, '/' );
          for (let from of dep.usingFroms)
            signal( error`Cannot read file '${resolved}'`, from.location );
        }
        else if (/^\.\.?\//.test( dep.module ) ) {
          for (let from of dep.usingFroms)
            signal( error`Cannot find module '${dep.module}'`, from.location );
        }
        else {
          for (let from of dep.usingFroms)
            signal( error`Cannot find module - do you mean './${dep.module}'?`, from.location );
        }
        return false;
      });
  }
}

// Synchronous version of function `compile` with limited functionality:
//  - option `--follow-deps` is not supported,
//  - an invocation error ends the compilation immediately.
function compileSync( filenames, dir='', options ) {
  dir = path.resolve(dir);
  var a = processFilenames(filenames, dir );
  var sources = Object.create(null);
  if (a.repeated.length)
    throw new InvocationError(a.repeated);

  try {
    for (let filename of a.files) {
      let source = fs.readFileSync( filename, 'utf8' );
      sources[ a.sources[filename] ] = source;
    }
  }
  catch (e) {
    throw new InvocationError( [e] );
  }
  return compileSources( sources, options );
}

// Promise-less main functions: compile the given sources.

// Argument `sources` is a dictionary (it could actually be a ordinary object)
// mapping filenames to either source texts (string) or an AST-like augmented
// CSNs.  It could also be a simple string, which is then considered to be the
// source text of a file named `<stdin>.cds`.
//
// See function `compile` for the meaning of the argument `options`.  If there
// are parse or other compilation errors, throw an exception CompilationError
// containing a vector of individual errors.
//
function compileSources( fileContentDict, options ) {
  if (typeof fileContentDict === 'string')
    fileContentDict = { '<stdin>.cds': fileContentDict };
  let sources = Object.create(null);
  var messagesArray = [];

  for (let filename in fileContentDict) {
    let source = fileContentDict[filename];
    if (typeof source === 'string') {
      let ast = parse( source, filename, options );
      sources[filename] = ast;
      ast.filename = filename;
      if (ast && ast.messages)
        messagesArray.push( ast.messages );
    }
    // else
    //   sources[filename] = source;
  }
  return compileDo( sources, messagesArray, options, fileContentDict );
}

function compileDo( sources, messagesArray, options = {}, fileContentDict ) {
  var model = { sources, options, messages: [].concat( ...messagesArray ) }; // flatten
  if (!options.testMode)
    model.version = versionObject( options );
  if (!options.parseOnly)
    model = resolve( define(model) );
  if (options.reAugmented) {    // for testing
    let filename = Object.keys(sources)[0];

    let source = compactJson( model, options );

    // parse stringified source to get the locations
    let jlParser = require("./json/parse")
    let locations = jlParser.parse( JSON.stringify(source), filename );
    source.locations = locations;
    source.$frontend = 'json';

    augmentor.augment( source, filename );
    model = { sources: Object.create(null), options, messages: [] };
    model.sources[filename] = source;
    if (options.reAugmented === 'recompile')
      model = resolve( define(model) );
  }
  semanticChecks(model);
  handleMessages( model );

  if (!options.disablePropagate) // TODO: remove propagate from compile() - with next CSN version increase
    model = propagator.propagateAssignments( model );
  if (!options.modelExtender || '$draft.cds' in sources)
    return model;

  let draft = options.modelExtender( model );
  if (!draft)
    return model;
  if (typeof draft === 'string')
    return compileSources( Object.assign( { '$draft.cds': draft }, fileContentDict ),
                           options );
  handleMessages( draft );
  throw new Error( 'Option `modelExtender` returns illegal value' );
}

function handleMessages( model ) {
  if (model.messages && model.messages.length) {
    model.messages = sortMessages( model.messages );
    if (hasErrors( model.messages ))
      throw new CompilationError( model.messages, model );
  }
}

// Process an array of `filenames`.  Returns an object with properties:
//  - `sources`: dictionary which has a filename as key (value is irrelevant)
//  - `files`: the argument array without repeating the same name
//  - `repeated`: array of filenames which have been repeatedly listed
//    (listed only once here even if listed thrice)
//
// Note: there is nothing file-specific about the filenames, the filenames are
// not normalized - any strings work
function processFilenames( filenames, dir ) {
  var sources = Object.create(null); // not {} = no [[Prototype]]
  var files = [];
  var repeated = [];
  for (let origname of filenames) {
    let name = path.resolve( dir, origname );
    if (sources[name] == null) {
      sources[name] = path.relative( dir, name );
      files.push(name);
    }
    else if (typeof sources[name] === 'string') { // not specified more than twice
      let msg = 'Repeated argument: file \'' + sources[name] + '\'';
      repeated.push( new ArgumentError( name, msg ) );
      //sources[name] = true;
    }
  }
  return { sources, files, repeated };
}

function exportAnnotations(model) {
  return backends.toI18n(model, { toI18n : { style : 'prop' } });
}

function exportAnnosUi5Style(model) {
  return backends.toI18n(model, { toI18n : { style : 'ui5' } });
}

function toSwagger(model) {
  return backends.toSwagger(model);
}

function generateExtensions(source, options = {}) {
  let content = options.toExtensions ?
    fs.readFileSync(source).toString()
    : source;
  let srcLoc = options.toExtensions ? source : '<code>';
  return generateExts(content, srcLoc);
}

// TNT-specific, temporary: Transforms augmented CSN 'model' into an object '{ annotations, metadata, csn, alerts, services }'
// containing
// FIXME: Please document what callers should expect in 'alerts'
// - 'annotations': (for backward compatibility only): the 'annotations' property of the first entry in 'services'
// - 'metadata': (for backward compatibility only): the 'metadata' property of the first entry in 'services'
// - 'csn': a transformed model
// - 'services': a dictionary of service names, containing for each 'service' from 'model' an object with
//   - 'annotations': an XML string with EDMX for ODATA v4 annotations for 'service'
//   - 'metadata': an XML string with EDMX for ODATA v2 metadata for 'service'
// The optional 'options' may contain TNT-specific settings to control
// the behavior of 'tntFlavor'. Default is to perform all TNT-specific post-processings,
// which equals to { tntFlavor: true }.
// FIXME: All transformations involved are not yet properly defined and are currently
// specific to the requirements of the TNT project (subject to change).
// Throws a CompilationError on errors.
function toTntSpecificOutput(model, options) {
  // If no 'options' are given, the default is to do full 'tntFlavor' without
  // skipping anything (for downward compatibility, behaves like {tntFlavor: true} ).
  // Selected aspects of 'tntFlavor' can be skipped by setting single options to 'true'.
  // Everything can be skipped by setting options to { tntFlavor: false }
  
  // Merge options from model (generate for ODATA what is required by TNT)
  options = mergeOptions({ toOdata : { version : 'v2', xml: true, separate : true, csn : true }, odataVersion : 'v2' }, model.options, options);

  // Whatever we now have as options, tntFlavor is now set simply because we are in this function
  // (just don't overwrite options.tntFlavor with defaults if we already got an object)
  if (!options.tntFlavor || typeof options.tntFlavor != 'object') {
    options.tntFlavor = tntSpecific.getDefaultTntFlavorOptions();
  }

  // Delegate to new API
  let odataResult = backends.toOdata(model, options);

  // Assemble result object
  let result = {
    csn: odataResult.csn,
    services: odataResult.services,
    alerts: odataResult.messages,
  }

  // FIXME: For backward compatibility, overwrite the CSN output with the TNT-specific version
  result.csn = tntSpecific.postProcessForTnt4allEntity(odataResult._augmentedCsn);

  // FIXME: For backward compatibility, replace the 'annotations.xml' in all services with the V4 version
  // (unfortunately we used to deliver this really as a V4 version, which was probably unnecessary ...)
  for (let serviceName in result.services) {
    let forOdata = transformModel.postProcessForBackwardCompatibility(odataResult._augmentedCsn, serviceName);
    let l_annotations_edm = csn2edm(forOdata, mergeOptions(options, { toOdata : { version : 'v4' }, odataVersion : 'v4' }));
    result.services[serviceName].annotations = l_annotations_edm.toXML('annotations');
  }

  // For backward compatibility, fill 'annotations' and 'metadata'
  // if there is exactly one service
  // FIXME: For backward compatibility only, should be removed soon
  let serviceNames = Object.keys(result.services);
  if (serviceNames.length == 1) {
    let firstService = result.services[serviceNames[0]];
    result.annotations = firstService.annotations;
    result.metadata = firstService.metadata;
  }
  return result;
}

// Transform augmented CSN `model` to OData using `options`.
// Argument `options` is an JS object with the following properties relevant
// for the transformation
// - `odataVersion`: with values 'v2' or 'v4', default: 'v4' (if not already
//   provided by the `options` property of argument `model`)
//
// The result is an object '{ annotations, metadata, combined, csn }'
// containing
// - annotations: an XML string with EDMX for ODATA v2/v4 annotations from 'model'
// - metadata: an XML string with EDMX for ODATA v2/v4 (depends on option) metadata from 'model'
// - combined: an XML string with EDMX for ODATA metadata + OData annotations from 'model'
// - csn: a transformed version of 'model' with the following transformation steps:
//
// Transformation steps for ODATA:
// - Flatten structured elements (and foreign keys of managed associations pointing to
//   keys that are themselves managed associations).
// - Generate foreign key fields for entities with managed associations (annotated with
//   '@odata.foreignKey4'). Propagate along projections accordingly. Names are built using
//   <assoc>_<key>, conflicts are checked.
// - Complete the 'foreignKeys' property for all managed associations, so that there
//   is always a 'generatedFieldName' for the corresponding generated foreign key field.
// - Implicitly redirect associations based on exposure
// - Check that exposed associations do not point to non-exposed targets
// - Unravel derived type chains, propagating annotations upwards.
// - Rename annotations according to a fixed list of short-hands
//
// FIXME: Please document 'metadata_json' and 'serviceName' and 'services' ...
// FIXME: Remove 'alerts', add `messages` from model (are warnings)
// FIXME: All transformations involved are not yet properly defined and are subject to change).
// FIXME: Please document 'odataOmitRecordType'
//
// Throws a CompilationError on errors.
function toOdataOutput( model, options ) {
  // For backward compatibility, take options 'true/false' as odataVersion V4 resp. v2
  if (typeof options === 'boolean') {
    options = { odataVersion: options ? 'v4' : 'v2' };
  }

  // Merge options from model (generate everything)
  options = mergeOptions({ toOdata : { xml: true, json : true, separate : true, combined : true, csn : true } }, model.options, options);

  // For backward compatibility, translate 'odataVersion' to 'toOdata.version'
  if (options.odataVersion) {
    options.toOdata.version = options.odataVersion;
  }

  // But do not generate JSON for ODATA V2
  // (Note that we do not report an error here for 'options.toOdata.json' with V2 but simply discard it. We would need 
  // more complex option merges to check that and I simply couldn't be bothered ... - the new API's 'toOdata' options are
  // not really expected to be set here from outside anyway)
  if (options.toOdata.version == 'v2') {
    options.toOdata.json = false;
  }

  // Delegate to new API
  let result = backends.toOdata(model, options);

  // For backward compatibility, convert the metadata JSONs to strings
  for (let serviceName in result.services) {
    result.services[serviceName].metadata_json = JSON.stringify(result.services[serviceName].metadata_json, null, 2);
  }

  // For backward compatibility, replace 'messages' with 'alerts'
  result.alerts = result.messages;
  delete result.messages;

  // For backward compatibility, fill 'annotations', 'metadata', 'metadata_json', 'combined', 'serviceName'
  // if there is exactly one service
  // FIXME: For backward compatibility only, should be removed soon
  let serviceNames = Object.keys(result.services);
  if (serviceNames.length == 1) {
    let firstService = result.services[serviceNames[0]];
    result.annotations = firstService.annotations;
    result.metadata = firstService.metadata;
    result.metadata_json = firstService.metadata_json;
    result.combined = firstService.combined;
    result.serviceName = serviceNames[0];
  }
  return result;
}

// Return the 'version' object that should appear in CSNs generated by this compiler.
function versionObject( options ) {
  return {
    creator: 'CDS Compiler v' + version(),
    csn: csnVersion( options ),
  }
}

// Class for command invocation errors.  Additional members:
//  `errors`: vector of errors (file IO or ArgumentError)
class InvocationError extends Error {
  constructor(errs,...args) {
    super(...args);
    this.errors = errs;
  }
}

// Class for argument errors.  Additional members:
//  `argument`: the command argument (repeated file names)
class ArgumentError extends Error {
  constructor(arg,...args) {
    super(...args);
    this.argument = arg;
  }
}

function parseToCqn( cdl, filename = '<query>.cds', options = {} ) {
  let xsn = parseLanguage( cdl, filename, options, 'query' );
  handleMessages( xsn );
  return compactQuery( xsn );
}

function parseToExpr( cdl, filename = '<expr>.cds', options = {} ) {
  let xsn = parseLanguage( cdl, filename, options, 'expr' );
  handleMessages( xsn );
  return compactExpr( xsn );
}

// FIXME: The implementation of those functions that delegate to 'backends' should probably move here
module.exports = {
  // Compiler
  version,
  parse,
  compile,                      // main function
  compileSync,                  // main function
  compileSources,               // main function
  compactJson,                  // should rather use toCsn
  compactSortedJson,            // should rather use toCsn
  compactModel,
  CompilationError,
  messageString,
  InvocationError,
  hasErrors,

  // Backends
  toHana : backends.toHana,
  toOdata : backends.toOdata,
  toCdl : backends.toCdl,
  toSwagger,
  toSql : backends.toSql,
  toI18n : backends.toI18n,
  toCsn : backends.toCsn,

  // additional API:
  parseToCqn,
  parseToExpr,

  // Everything below is for backward compatibility only and should no longer be used
  exportAnnotations,            // Replaced by toI18n (style 'prop')
  exportAnnosUi5Style,          // Replaced by toI18n (style 'ui5')
  generateExtensions,           // Should be removed
  toSqlDdl : backends.toSql,    // Just another name
  forHana : forHana.transformForHana, // Just another name
  toHanaCdl : backends.toHana,  // Just another name
  toTntSpecificOutput,          // FIXME: Temporary, subject to change
  toOdataOutput                // FIXME: Temporary, subject to change
}
